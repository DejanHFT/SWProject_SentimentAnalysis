Title,Author,URL,Number of Comments,Upvotes,Text Content
"Utilitarianism - Weaknesses, and possible future directions.",electronics12345,https://www.reddit.com/r/philosophy/comments/53wgsz/utilitarianism_weaknesses_and_possible_future/,44,7,"Just as a preface, I am a strong proponent of Utilitarianism, and I submit this as a means to improve upon a theory which already has a lot going for it. 

Utilitarianism as a moral theory posits 2 maxims. First, that each person has a well defined utility and utility function. The utility function describes how a persons utility would change under differing circumstances. Second, that the correct moral decision is the option which maximizes utility. 

The first glaring weakness, is that determining a person's utility function is quite difficult if not impossible. While married persons and best friends can know each other quite well, even such persons can be surprised by each other every once in a while. As for determining a stranger's utility, we are largely in the dark. People's utility functions don't hover above them like health bars in a video game. 

Second, is the issue of scaling. Let's say it was hypothetically possible to know a person's utility on a scale from 1 to 10. Would it make sense to sum those values? Is it possible to have negative utility? Would it make sense to transform that 1 to 10 scale to a -5 to 5 scale before doing the summation? Is it possible (as some anti-natalists believe) that we are all suffering and that utility should be scaled from -10 to -1. This question may seem trivial, but it has reaching effects. If someone has a 2 on the 1-10 scale, do they have a right to kill themselves? If the 1-10 scale is accurate 2>0, so they ought not. If the -5 to 5 scale is accurate -3 <0, so they ought to have that right. Which is better: a society with 100 10s or a society with a billion 1s. If we rescale, we get a society with 100 5s vs a society with a billion -5s. If the number of people involved is the same, rescaling doesn't really matter. But, when we are contemplating death/abortion/societal revolution/war/torture, it has major impact on our conclusions. 

In short, Utilitarianism has 2 maxims: that each person has a utility function (which we cannot know) and that we ought to maximize the sum (addition is dependent on scaling factors which makes addition fundamentally arbitrary when dealing with groups of uneven size). 

As far as addressing these issues, I propose using Bayesian-style reasoning as a basis for estimating utility functions. Bayesian statistics is the idea that there is some background information which is true generally (the prior) but that new information can be gained about specific cases (the evidence) which can be combined to create a new estimate (the posterior). As a prior, morality has a number of good options: The Veil of Ignorance, polls of the population (regarding preferences), and intuition just to name a few. While these are important, I think it would be insane to throw away known information about particular individuals if we happen to know it. Morality is best informed when we account for information about humanity overall (humans dislike pain, humans like simple sugars) but also information about particular actors. 

My second suggestion directly appeals to ""the repugnant conclusion"". The idea of this argument is that no society of 100 persons will ever have more utility than a society of billions with utility of 0.001. My suggestion would be to set the scaling for utility such that this conclusion is no longer repugnant. In other words ""to set the 0"" of the utility scale such that if everyone in society had a 0.001 utility and there were billions of people, it would be a better society than 100 maximally happy persons. 

Last, no discussion of the weaknesses of Utilitarianism can be had without mention of ""The Utility Monster"". First, in general arguments about Utility Monsters, or P-Zombies or Descartes Demons I feel can be taken with a grain of salt. These are extraordinary claims, which can be dismissed in the absence of evidence. Second, in a sense, utility monsters are real, they are called the poor. Persons in the Third World could gain enormous utility from very little expense (from a First World perspective). The sheer volume of possibility of utility increase in the Third World dwarfs the possible gains in the First. In this way, I think Effective Altruism and Utilitarianism are quite compatible. Spending funds optimally to get the most utility is in the interest of the EA and the Utilitarian. Third, I think framing Utilitarianism in terms of EA-type scenarios makes it more palatable than the trolley problem which makes Utilitarians look evil or uncaring. If someone could really use $20 and you don't really need it, donating it is seen as charitable, while killing the 1 instead of the 5 is not seen in the same light, even though the same reasoning is behind it. 

Thank you for your consideration. Please thoroughly slaughter this so I can learn. "
